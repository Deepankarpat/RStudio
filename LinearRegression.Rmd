---
title: "Assignment_1"
author: "Deepankar Pattnaik"
date: "6/11/2019"
output: html_document
---

### (a)
```{r}

library(ISLR)
head(Auto)
pairs(Auto, pch = 21)

```

### pairs() function is used to produce scatter plot matrix of all the variables in the data set. Some trends in the data can be noticed like with increase in vehicle 'weight', the fuel efficiency decreases. Also with increase in number of cylinders the fuel efficiency either increases or decreases.Similarly it can noticed, for every new 'year' the value of 'mpg' is increasing i.e. the fuel efficency is improving with year.

### (b)
```{r}

cor(Auto[ , names(Auto) != "name"])
library(corrplot)

corrplot(cor(Auto[ , names(Auto) != "name"]), type = "lower")
```

### The column 'name' has been removed as it is not numeric i.e. a qualitative variable. The signs and magnitudes shows the direction and strength of relationship between the varibales, when observed in the above scattered plot matrix. Though the results may be doubtful due to the non-linear relationships between some variables. 


### (c)
```{r}
model_1 = lm(mpg ~. -name, data = Auto)
summary(model_1)
```

### I. Yes, there is a relationship between the predictors and the response. With the Adjusted R-squared value 0.8181, we can say 81.82% changes in the response can be explained by this predictor variables. Also the p-value from F-statistic is much lower than 0.05 implying some relationship between the predictors and the response. 

### II. 'displacement', 'weight' , 'year' and 'origin' are considered to have a statistically significant relationship to the response variable 'mpg'

### III. For a unit increase in 'year' , we expect a 0.75 increase in 'mpg'. It suggests that 'mpg' improves over time due to technological advancement.We can conclude  new cars to be more fuel efficient.


### (d)
```{r}
par(mfrow=c(2,2)) # Change the panel layout to 2 x 2
plot(model_1)

```

### There is non-linearity present and residuals take on a slight bow shape indicating issues with the fit.
### The assumption of normality does not seem to be violated though the Normal Q-Q plot shows significant deviation in the upper tail.
### There does appear to be a number of residuals that suggest unusually large outliers as marked in the plots (387, 323, 394).
### Yes, there is an observation labeled 14 that stands out as a potential leverage point on the graph.


### (e)
### From the corrplot() diagram the interaction between 'displacement' and 'cylinder' , 'weight' and displacement', 'horsepower' and 'displacement' also 'weight' and 'cylinder' looked promising.
```{r}
model_2 <- lm(mpg ~ . -name +displacement*cylinders  , data = Auto)
summary(model_2)
par(mfrow = c(2,2))
plot(model_2)

model_3 <- lm(mpg ~ . -name +weight*displacement , data = Auto)
summary(model_3)
plot(model_3)

model_4 <- lm(mpg ~ . -name +displacement*horsepower , data = Auto)
summary(model_4)
plot(model_4)

model_5 <- lm(mpg ~ . -name +weight*cylinders , data = Auto)
summary(model_5)
plot(model_5)

model_6 <- lm(mpg ~ . -name -displacement  +weight*cylinders  +acceleration*horsepower +year*origin , data = Auto)
summary(model_6)
plot(model_6)


```

### We can notice the interection effect is significant and the Adjusted R-squared value has increased, implying a better fit for all the models when compared to 'model_1'. Many different possible ways are there to improve your model. Here , the last model i.e. 'model_6' has all predictor variables to be highly significant and the R-squared value suggest, more than 86%  response can be explained by this perdictors (single & interaction). 
###  QQ plot where the residuals deviate from the diagonal line in both the upper and lower tail. This plot indicated that the tails are ‘lighter’ (have smaller values) than what we would expect under the standard modeling assumptions. This is indicated by the points forming a “flatter” line than than the diagonal.

### (f)
```{r}
model_t1 <- lm(mpg ~ . -name -cylinders -displacement +log(weight) + sqrt(acceleration) + I(horsepower^2) , data = Auto)
summary(model_t1)
par(mfrow=c(2,2))
plot(model_t1)

model_t2 <- lm(mpg ~ . -name  +log(weight) + sqrt(horsepower) + I(displacement^2) , data = Auto)
summary(model_t2)
plot(model_t2)

model_t3 <- lm(mpg ~ . -name -displacement  +log(horsepower) + sqrt(weight) + I(acceleration^2) , data = Auto)
summary(model_t3)
plot(model_t3)

```

### It can be noticed that with the transformations of the predictor variables, the R-squared value has increased indicating a better fit. Also some of the interaction variables are of high significance.  'model_t3' has the highest R-squared value i.e. 0.865 and also the preditor variables (single & transformed) are significant implying a better model.

### Also from the residual graphs it is known that this linear model is good fit for relatively small x values, but is not a good predictor of larger x values. The distribution of the residuals is quite well concentrated around 0 for small fitted values, but they get more and more spread out as the fitted values increase.This is an instance of “increasing variance”. The standard linear regression assumption is that the variance is constant across the entire range. When this assumption isn’t valid, such as in this example, we shouldn’t believe our confidence intervals, prediction bands, or the p-values in our regression.
